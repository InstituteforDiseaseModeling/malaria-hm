{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malaria challenge trial calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A history matching approach to calibrating an empirical model of malaria infection and immunity\n",
    "### For iter0 we will cut down parameter space by using the distribution of malaria prevalence by season, age and density bins from a moderate high transmission setting in Sugungum, Garki, Nigeria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like you don't have CUDA, that's okay, we'll try using CPU but it will be SLOW!\n",
      "Looks like you don't have CUDA, that's okay, we'll try using CPU but it will be SLOW!\n"
     ]
    }
   ],
   "source": [
    "from IPython.extensions import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, glob, re, sys, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from calibtool.analyzers.Helpers import \\\n",
    "    convert_to_counts, age_from_birth_cohort, season_from_time, aggregate_on_index\n",
    "\n",
    "# from wand.image import Image as WImage\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "from pyDOE import lhs\n",
    "from history_matching import HistoryMatching, HistoryMatchingCut, quick_read, Basis\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_glob(pth):\n",
    "    return re.sub('([\\[\\]])','[\\\\1]', pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure parameters and data for history matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__copy__', '__deepcopy__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'end', 'endpos', 'expand', 'group', 'groupdict', 'groups', 'lastgroup', 'lastindex', 'pos', 're', 'regs', 'span', 'start', 'string']\n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not '_sre.SRE_Match'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bbc0f001dc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'iter(\\d+)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'iter(\\d+)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Index of the current iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#list of exp_ids/folders that correspond to exp_ids from which our emulation will draw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mexp_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'd8e821ca-c300-ea11-a2c3-c4346bcb1551'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# TODO: only uses first for now, should loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not '_sre.SRE_Match'"
     ]
    }
   ],
   "source": [
    "age_bin_query = 1\n",
    "density_bin_query = 200\n",
    "cut_name = f'pfpr_agebin{age_bin_query}_and_densitybin{density_bin_query}'\n",
    "iteration = int(re.search(r'iter(\\d+)', os.getcwd()).groups()[0])\n",
    "\n",
    "\n",
    "#list of exp_ids/folders that correspond to exp_ids from which our emulation will draw\n",
    "exp_ids = ['d8e821ca-c300-ea11-a2c3-c4346bcb1551'] # TODO: only uses first for now, should loop\n",
    "basedir = os.path.join('C:', 'git', 'Malaria-Uganda-PRISM')\n",
    "datafile = os.path.join('..','reference data', 'Garki_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implausibility threshold determines how willing we are to retain regions\n",
    "# of parameter space that are inconsistent with the underlying data. A higher\n",
    "# threshold is more risk averse in that potentially good regions are less likely\n",
    "# to be rejected, however it will take more iterations/simulations to achieve results.\n",
    "implausibility_threshold = 3\n",
    "training_fraction = 0.75 # Fraction of simulations to use as training\n",
    "discrepancy_std = 0.0 # Accounts for uncertainty w.r.t model structure\n",
    "n_samples_to_generate_for_next_iter = 1000 # Number of simulations to conduct on this iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the parameter names and ranges\n",
    "params_file = os.path.join('..','Params.xlsx')\n",
    "param_info = quick_read(params_file, 'Params').set_index('Name')\n",
    "param_names = param_info.index.tolist()\n",
    "\n",
    "params = param_info.index.values\n",
    "n_params = param_info.shape[0] # We'll use this one place later\n",
    "display(param_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Sim Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "all_results = []\n",
    "for exp_id in exp_ids:\n",
    "    \n",
    "    print('-'*80, '\\nExperiment:', exp_id)\n",
    "    sim_results_fn = os.path.join(exp_id, 'analyzer_results.csv')\n",
    "    print(sim_results_fn)\n",
    "    sr = pd.read_csv(sim_results_fn, skipinitialspace=True)\n",
    "    sr['Sample_Id'] = sr['sample'].apply(lambda x: '%s.%06d' % (exp_id, x))\n",
    "    sr.rename(columns = {'sim_id': 'Sim_Id','value':'Result'}, inplace=True)\n",
    "    all_results.append( sr )\n",
    "    #read in tags making a samples.xlsx file \n",
    "    s = pd.read_excel(os.path.join(exp_id, 'Samples.xlsx'))\n",
    "    # s.drop('Sim_Id', axis=1, inplace=True)\n",
    "    s['Sample_Id'] = s['Sample'].apply(lambda x: '%s.%06d' % (exp_id, x))\n",
    "    all_samples.append( s )\n",
    "\n",
    "#.rename(columns={'level_1': 'Year', 0: 'Cases'}) \\\n",
    "all_results = pd.concat(all_results) \\\n",
    "    .set_index(['Sample_Id', 'Sim_Id']) \\\n",
    "    [['Result']] \\\n",
    "    .sort_index()\n",
    "\n",
    "samples = pd.concat(all_samples).set_index('Sample_Id').sort_index() # Bad because sample will be repeated across exp_id!\n",
    "                       \n",
    "samples.to_csv('Samples.csv')\n",
    "all_results.to_csv('Results.csv')\n",
    "results = all_results['Result']\n",
    "\n",
    "results = all_results \\\n",
    "    .groupby(['Sample_Id', 'Sim_Id']) \\\n",
    "    .mean()\n",
    "\n",
    "    \n",
    "results.rename(columns={'Result':'Sim_Result'}, inplace=True)\n",
    "\n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_df(df, pfprdict, index, column_keep, column_del):\n",
    "    \"\"\"\n",
    "    Recut dataframe to recategorize data into desired age and parasitemia bins\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe to be rebinned\n",
    "        pfprdict: Dictionary mapping postive counts per slide view (http://garkiproject.nd.edu/demographic-parasitological-surveys.html)\n",
    "                to density of parasites/gametocytes per uL\n",
    "        index: Multi index into which 'df' is rebinned\n",
    "        column_keep: Column (e.g. parasitemia) to keep\n",
    "        column_del: Column (e.g. gametocytemia) to delete\n",
    "    \"\"\"\n",
    "    dftemp = df.copy()\n",
    "    del dftemp[column_del]\n",
    "\n",
    "    dftemp['PfPR Bin'] = df[column_keep]\n",
    "    dftemp = aggregate_on_index(dftemp, index)\n",
    "\n",
    "    dfGrouped = dftemp.groupby(['Season', 'Age Bin', 'PfPR Bin'])\n",
    "\n",
    "    dftemp = dfGrouped[column_keep].count()\n",
    "    dftemp = dftemp.unstack().fillna(0).stack()\n",
    "    dftemp = dftemp.rename(column_keep).reset_index()\n",
    "    dftemp['PfPR Bin'] = [pfprdict[p] for p in dftemp['PfPR Bin']]\n",
    "\n",
    "    dftemp = dftemp.set_index(['Season', 'Age Bin', 'PfPR Bin'])\n",
    "\n",
    "    return dftemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_data_path = os.path.join('..','reference data','Garki_df.csv')\n",
    "df = pd.read_csv(ref_data_path)\n",
    "\n",
    "self = pd.DataFrame()\n",
    "self.metadata =  {\n",
    "        'density_bins': [0, 50, 200, 500, np.inf],  # (, 0] (0, 50] ... (50000, ]\n",
    "        'density_bin_edges':['0', '50', '200', '500'],\n",
    "        'age_bins': [0, 1, 4, 8, 18, 28, 43, np.inf],  # (, 5] (5, 15] (15, ],\n",
    "        'age_bin_labels':['<1', '1-4', '4-8', '8-18', '18-28', '28-43', '>43'],\n",
    "        'seasons': ['DC2', 'DH2', 'W2'],\n",
    "        'seasons_by_month': {\n",
    "            'Apr': 'DH2',\n",
    "            'June/Aug': 'W2',\n",
    "            'Dec/Jan': 'DC2'\n",
    "        },\n",
    "        'village': 'Sugungum'\n",
    "    }\n",
    "\n",
    "df = df.loc[df['Village']==self.metadata['village']]\n",
    "pfprBinsDensity = self.metadata['density_bins']\n",
    "uL_per_field = 0.5 / 200.0  # from Garki PDF - page 111 - 0.5 uL per 200 views\n",
    "pfprBins = 1 - np.exp(-np.asarray(pfprBinsDensity) * uL_per_field)\n",
    "seasons = self.metadata['seasons']\n",
    "pfprdict = dict(zip(pfprBins, pfprBinsDensity))\n",
    "\n",
    "bins = OrderedDict([\n",
    "    ('Season', self.metadata['seasons']),\n",
    "    ('Age Bin', self.metadata['age_bins']),\n",
    "    ('PfPR Bin', pfprBins)\n",
    "])\n",
    "bin_tuples = list(itertools.product(*bins.values()))\n",
    "index = pd.MultiIndex.from_tuples(bin_tuples, names=bins.keys())\n",
    "\n",
    "df = df.loc[df['Seasons'].isin(seasons)]\n",
    "df = df.rename(columns={'Seasons': 'Season', 'Age': 'Age Bin'})\n",
    "\n",
    "df2 = grouped_df(df, pfprdict, index, 'Parasitemia', 'Gametocytemia')\n",
    "df3 = grouped_df(df, pfprdict, index, 'Gametocytemia', 'Parasitemia')\n",
    "dfJoined = df2.join(df3).fillna(0)\n",
    "dfJoined = pd.concat([dfJoined['Gametocytemia'], dfJoined['Parasitemia']])\n",
    "dfJoined.name = 'Counts'\n",
    "dftemp = dfJoined.reset_index()\n",
    "dftemp['Channel'] = 'PfPR by Gametocytemia and Age Bin'\n",
    "dftemp.loc[len(dftemp) / 2:, 'Channel'] = 'PfPR by Parasitemia and Age Bin'\n",
    "dftemp = dftemp.rename(columns={'Seasons': 'Season', 'PfPR Bins': 'PfPR Bin', 'Age Bins': 'Age Bin'})\n",
    "dftemp = dftemp.set_index(['Channel', 'Season', 'Age Bin', 'PfPR Bin'])\n",
    "\n",
    "# how to set the cwd\n",
    "ref_data =dftemp.copy(deep = True)\n",
    "ref_data['bin_pop'] = ref_data.groupby(by=['Channel', 'Season', 'Age Bin'])['Counts'].sum()\n",
    "ref_data['proportion'] = ref_data['Counts'] / ref_data['bin_pop']\n",
    "ref_data.reset_index(inplace=True)\n",
    "a = ref_data[(ref_data['PfPR Bin'] != 0) &(ref_data['Channel'] == 'PfPR by Parasitemia and Age Bin')]\n",
    "b = a[(a['Age Bin'] == age_bin_query) & (a['PfPR Bin'] == density_bin_query)]\n",
    "desired_result = np.mean(b.proportion)\n",
    "desired_result_std = 0.025 #np.nan\n",
    "print('Before logit, desired result is %.3f [%.3f, %.3f]: ' % (desired_result, desired_result-2*desired_result_std, desired_result+2*desired_result_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.merge(samples, results, on='Sample_Id') # Return to Sample_Id\n",
    "\n",
    "#sub in variables here, can be list of tuples\n",
    "xyvars = [('Cumulative_Exposure_Immune_Coefficient_PPP',\t'Cumulative_Exposure_Immune_Coefficient_TM'\n",
    ")]\n",
    "for (xvar, yvar) in xyvars:\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    plt.scatter(z[xvar], z[yvar], c=100*z['Sim_Result'], s=50*z['Sim_Result'], cmap='jet')\n",
    "    plt.xlabel(xvar); plt.ylabel(yvar)\n",
    "    plt.xlim([param_info.loc[xvar, 'Min'], param_info.loc[xvar, 'Max']])\n",
    "    plt.ylim([param_info.loc[yvar, 'Min'], param_info.loc[yvar, 'Max']])\n",
    "    # TODO: Show as surface and plot desired result as isocline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we get to do some History Matching!\n",
    "# Begin by creating an instance of the HistoryMatching class\n",
    "ext = 'svg' # Filetype for figures produced by history matching\n",
    "hm = HistoryMatching(\n",
    "    cut_name = cut_name,\n",
    "    param_info = param_info,\n",
    "    inputs = samples,\n",
    "    results = results.squeeze(),\n",
    "    desired_result = desired_result,\n",
    "    desired_result_var = desired_result_std**2,\n",
    "    iteration = iteration,\n",
    "    implausibility_threshold = implausibility_threshold,\n",
    "    discrepancy_var = discrepancy_std**2,\n",
    "    training_fraction = training_fraction,\n",
    "    fig_type = ext\n",
    ")\n",
    "hm.save() # Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    existing_basis = True\n",
    "    need_basis = True\n",
    "    with open(os.path.join('Cuts', cut_name, 'basis_glm.json')) as data_file:\n",
    "        config = json.load( data_file )\n",
    "        basis_glm = Basis.deserialize(config['Basis'])\n",
    "        fitted_values = pd.read_json(config['Fitted_Values'], orient='split').set_index(['Sample_Id', 'Sim_Id']).squeeze()\n",
    "except:\n",
    "    existing_basis = False\n",
    "    \n",
    "if existing_basis:\n",
    "    print('Found existing GLM basis with the following terms:')\n",
    "    display(basis_glm.model_terms)\n",
    "    reply = input('Would you like to use this basis? [Y]/n: ')\n",
    "    \n",
    "    if reply.lower() != 'n':\n",
    "        need_basis = False\n",
    "    \n",
    "if need_basis:\n",
    "    basis_glm = Basis.polynomial_basis(params=param_names, intercept = True, first_order=True, second_order=True, third_order=False, param_info=param_info)\n",
    "\n",
    "    basis_glm.plot_regularize(samples, results, alpha = np.logspace(-6, -3, 15), scaleX=True)\n",
    "    alpha_glm = float(input('What would you like to use for the GLM regularization parameter, alpha_glm = '))\n",
    "#     alpha_glm = 1e-1\n",
    "    \n",
    "    fitted_values = basis_glm.regularize(samples, results, alpha = alpha_glm, scaleX=True)\n",
    "    print('Regularization for GLM selected:\\n', ' *','\\n * '.join(basis_glm.get_terms()))\n",
    "    with open(os.path.join('Cuts', cut_name, 'basis_glm.json'), 'w') as fout:\n",
    "        json.dump( {\n",
    "            'Basis': basis_glm.serialize(),\n",
    "            'Fitted_Values': fitted_values.reset_index().to_json(orient='split')\n",
    "        }, fout, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit the glm and plot\n",
    "\n",
    "### GLM ###############################################################\n",
    "print(\"=\"*80, \"\\nGeneralized Linear Modeling\\n\", \"=\"*80)\n",
    "#######################################################################\n",
    "f = hm.glm(\n",
    "    basis = basis_glm,\n",
    "    family = 'Poisson',\n",
    "    force_optimize_glm = True,\n",
    "    glm_fit_maxiter = 1000,\n",
    "    plot = True, #force_optimize_glm,\n",
    "    plot_data = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(fix_glob(os.path.join(hm.glmdir, \"PairwiseResults\", \"*.%s\"%ext))):\n",
    "    img = IFrame(file, width=800, height=600)\n",
    "    print(file)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results get saved to disk, so load and display:\n",
    "for file in glob.glob(fix_glob(os.path.join(hm.glmdir, \"GLM Predicted vs Actual*.%s\"%ext))):\n",
    "    img = IFrame(file, width=800, height=600)\n",
    "    print(file)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_gpr = Basis.polynomial_basis(\n",
    "    params =  ['Scale_Factor_Age_a', 'Scale_Factor_Age_b','Biological_Age_Immune_Coefficient_PPP',\n",
    "'Biological_Age_Immune_Coefficient_TM',\n",
    "'Cumulative_Exposure_Immune_Coefficient_PPP',\n",
    "'Cumulative_Exposure_Immune_Coefficient_TM'\n",
    "              ],\n",
    "#     params = param_info.index.values, #\n",
    "    intercept = False, \n",
    "    first_order = True, \n",
    "    param_info=param_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### GPR ###############################################################\n",
    "print(\"=\"*80, \"\\nGaussian Process Regression\\n\", \"=\"*80)\n",
    "#######################################################################\n",
    "hm.gpr(\n",
    "    basis = basis_gpr,\n",
    "    force_optimize_gpr = True,\n",
    "\n",
    "    sigma2_f_guess = 0.6,\n",
    "    sigma2_f_bounds = (0.1, 1000),\n",
    "    sigma2_n_guess =  2.0,\n",
    "    sigma2_n_bounds = (0.01, 100),\n",
    "\n",
    "    #lengthscale_guess = [0.09844299, 0.1256657, 0.0976875, 0.09889085, 0.1051974, 0.0950809, 0.10032171, 0.10599185, 0.10627393, 0.09950996, 0.09445544, 0.10285915, 0.10007409, 0.09847433, 0.08963389, 0.10205652, 0.09360044, 0.1024141, 0.09786228, 0.10247492, 0.09852253, 0.09632744, 0.09997534, 0.10767302, 0.10095249, 0.09941825, 0.10214923, 0.10221497, 0.09734157, 0.09093285, 0.10780673, 0.09881377, 0.10597152],\n",
    "    lengthscale_guess = 0.25,\n",
    "    lengthscale_bounds = (0.01, 100),\n",
    "\n",
    "    optimize_sigma2_n = True,\n",
    "    log_transform = False,\n",
    "\n",
    "    verbose = True,\n",
    "    optimizer_options = {\n",
    "        'eps': 5e-3,\n",
    "        'disp': True,\n",
    "        'maxiter': 15000,\n",
    "        'ftol': 2 * np.finfo(float).eps,\n",
    "        'gtol': 2 * np.finfo(float).eps,\n",
    "    },\n",
    "    plot = True, #force_optimize_gpr,\n",
    "    plot_data = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = param_info.loc[basis_gpr.param_dict.keys()].reset_index()\n",
    "pi['Lengthscale'] = hm.gpr_model.theta[2:]\n",
    "print(pi[['Name', 'Lengthscale']].sort_values('Lengthscale'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results get saved to disk, so load and display:\n",
    "for file in glob.glob(fix_glob(os.path.join(hm.gprdir, \"PairwiseResults\", \"*.%s\"%ext))):\n",
    "    img = IFrame(file, width=800, height=600)\n",
    "    print(file)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implausibility ############################################################\n",
    "print(\"=\"*80, \"\\nImplausibility\\n\", \"=\"*80)\n",
    "###############################################################################\n",
    "hm.calc_and_plot_implausibility(\n",
    "    plot = True,\n",
    "    do_plot_data = False,\n",
    "    plot_data_highlight = pd.DataFrame() #hm.test_data.loc['prime.000049']\n",
    ") \n",
    "    #plot_data_highlight=pd.DataFrame() # plot_data_highlight=hm.training_data.loc['prime.000049']\n",
    "\n",
    "hm.training_data.to_excel(os.path.join('Cuts', cut_name, 'train_data.xlsx'))\n",
    "hm.test_data.to_excel(os.path.join('Cuts', cut_name, 'test_data.xlsx'))\n",
    "\n",
    "print('Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results get saved to disk, so load and display:\n",
    "for file in glob.glob(fix_glob(os.path.join(hm.combineddir, \"*.%s\"%ext))):\n",
    "    img = IFrame(file, width=800, height=600)\n",
    "    print(file)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = all_results.reset_index()\n",
    "\n",
    "data = ar.loc[ar['AgeBin']==age_bin[0]].groupby(['Sample_Id','AgeBin'])['Result'].mean().reset_index().set_index('Sample_Id')\n",
    "\n",
    "train = hm.training_data\n",
    "train['Train'] = True\n",
    "test = hm.test_data\n",
    "test['Train'] = False\n",
    "train_test = pd.concat([train, test])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "# Reference\n",
    "#ref_by_year = reference_data[['Prevalence']].reset_index()\n",
    "\n",
    "ref_by_year = ref_df.loc[ref_df['Age bin'] == int(age_bin[0][:2])] \\\n",
    "    .groupby('Age bin') \\\n",
    "    .mean() \\\n",
    "    .reset_index()\n",
    "print(ref_by_year)\n",
    "print(data)\n",
    "sns.lineplot(data=ref_by_year, x='Age bin', y='Mean', color='k', marker='o', alpha=1, lw=2, zorder=1);\n",
    "\n",
    "# Sims\n",
    "tmp = data.merge(train_test[['Sample_Orig', 'Implausible', 'Train']], left_on='Sample_Id', right_on='Sample_Orig')\n",
    "print(tmp.head())\n",
    "sns.lineplot(data=tmp, x='AgeBin', y='Result', hue='Implausible', style='Train', \n",
    "             units='Sample_Orig', estimator=None, alpha=0.5, lw=0.5,\n",
    "             ax=ax, zorder=-1)\n",
    "ax.set_xlabel('AgeBin');\n",
    "ax.set_ylabel('Duration');\n",
    "ax.set_xticks(ref_by_year['Age bin'].unique());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
